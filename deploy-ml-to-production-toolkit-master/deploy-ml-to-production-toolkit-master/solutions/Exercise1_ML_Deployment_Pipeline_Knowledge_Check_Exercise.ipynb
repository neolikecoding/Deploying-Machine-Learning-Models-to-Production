{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following assumptions may impact a modelâ€™s generalizability?\n",
    "\n",
    "a) Data must be stationary (ex. no seasonality)\n",
    "\n",
    "b) Distribution of past data must be similar to unseen future data\n",
    "\n",
    "c) Train, Validation & Test examples are drawn from the same distribution \n",
    "\n",
    "d) Rows in each set should be independent of each other\n",
    "\n",
    "e) All of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "Answer is (e), All of the above. Most of the ML algorithms assume that both training and testing data are drawn from the same distribution. If the newly collected data has a different distribution, the trained ML model will no longer make predictions which make sense given this new distribution. \n",
    "\n",
    "It's important to ensure that train, validate & test set are drawn from same distribution. Consider for example if our training data contained only local flights while test data contained only international flights. Will the model have an ability to learn from features about international flights? No since the training set contained only local flights.\n",
    "\n",
    "Imagine if our entire dataset included flights of particular family members within a household. Any model we train using this data will only learn about members from this household which is highly correlated and biased (un-representative of other travelers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Suppose we train on training data, evaluated on test data, use the evaluation results from test data to do feature selection & hyper-parameter tuning. Are there any issue(s) with this methodology?\n",
    "\n",
    "a) Yes\n",
    "\n",
    "b) No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "Answer is (a). This concept is known as knowledge leak. The more we evaluate using the results from the test set, the more risk we take for overfitting to the test set. This is a primary motivation for having a out of time set to deal with this issue specifically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
